\documentclass{article}\usepackage[]{graphicx}\usepackage[]{color}
%% maxwidth is the original width if it is less than linewidth
%% otherwise use linewidth (to make sure the graphics do not exceed the margin)
\makeatletter
\def\maxwidth{ %
  \ifdim\Gin@nat@width>\linewidth
    \linewidth
  \else
    \Gin@nat@width
  \fi
}
\makeatother

\definecolor{fgcolor}{rgb}{0.345, 0.345, 0.345}
\newcommand{\hlnum}[1]{\textcolor[rgb]{0.686,0.059,0.569}{#1}}%
\newcommand{\hlstr}[1]{\textcolor[rgb]{0.192,0.494,0.8}{#1}}%
\newcommand{\hlcom}[1]{\textcolor[rgb]{0.678,0.584,0.686}{\textit{#1}}}%
\newcommand{\hlopt}[1]{\textcolor[rgb]{0,0,0}{#1}}%
\newcommand{\hlstd}[1]{\textcolor[rgb]{0.345,0.345,0.345}{#1}}%
\newcommand{\hlkwa}[1]{\textcolor[rgb]{0.161,0.373,0.58}{\textbf{#1}}}%
\newcommand{\hlkwb}[1]{\textcolor[rgb]{0.69,0.353,0.396}{#1}}%
\newcommand{\hlkwc}[1]{\textcolor[rgb]{0.333,0.667,0.333}{#1}}%
\newcommand{\hlkwd}[1]{\textcolor[rgb]{0.737,0.353,0.396}{\textbf{#1}}}%
\let\hlipl\hlkwb

\usepackage{framed}
\makeatletter
\newenvironment{kframe}{%
 \def\at@end@of@kframe{}%
 \ifinner\ifhmode%
  \def\at@end@of@kframe{\end{minipage}}%
  \begin{minipage}{\columnwidth}%
 \fi\fi%
 \def\FrameCommand##1{\hskip\@totalleftmargin \hskip-\fboxsep
 \colorbox{shadecolor}{##1}\hskip-\fboxsep
     % There is no \\@totalrightmargin, so:
     \hskip-\linewidth \hskip-\@totalleftmargin \hskip\columnwidth}%
 \MakeFramed {\advance\hsize-\width
   \@totalleftmargin\z@ \linewidth\hsize
   \@setminipage}}%
 {\par\unskip\endMakeFramed%
 \at@end@of@kframe}
\makeatother

\definecolor{shadecolor}{rgb}{.97, .97, .97}
\definecolor{messagecolor}{rgb}{0, 0, 0}
\definecolor{warningcolor}{rgb}{1, 0, 1}
\definecolor{errorcolor}{rgb}{1, 0, 0}
\newenvironment{knitrout}{}{} % an empty environment to be redefined in TeX

\usepackage{alltt}

\title{Multivariate logistic distribution}

\usepackage{amsmath}

\usepackage{natbib}
\bibliographystyle{chicago}
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\begin{document}

\maketitle

\section{Latent variable notation}

We are interested in modeling multivariate binomial outcomes. We will write the observed outcome as $\mathbf y_i = (y_{i1}, y_{i2}, \dots, y_{ip})'$, a vector of binary variables. To keep our notations consistent, we will model $\mathbf y_i$ using a latent variable approach:
\begin{equation}
\begin{aligned}
\mathbf y_i &= 1(\mathbf z_i > 0)\\
\mathbf z_i &\sim \mathrm{distrib}(\mu, \Sigma^2).
\end{aligned}
\end{equation}
where $\mathbf z_i$ is a vector of continuous variable following a probability distribution with mean $\mu$ and covariance $\Sigma^2$.
This approach generalizes the univariate logistic regression.

\section{Models}

\subsection{Logistic regression}

First, consider the univariate logistic regression (with intercept only):
\begin{equation}
\textrm{logit Pr}(y_i = 1) = \mu.
\end{equation}
This is equivalent to writing
\begin{equation}
\begin{aligned}
\mathbf y_i &= 1(\mathbf z_i > 0)\\
\mathbf z_i &\sim \mathrm{logistic}(\mu, 1),
\end{aligned}
\end{equation}
where $\mathrm{logistic}(\cdot|\mu, s)$ is a logistic \emph{distribution} with location parameter $\mu$ and a scale parameter $s$.

One way to extend this model to account for multivariate observation is to use a mixed model approach: 
\begin{equation}
\begin{aligned}
\textrm{logit Pr}(\mathbf Y_i = y_i) &= \pmb \mu + \mathbf r_i\\
\mathbf r_i &\sim \mathcal{N}(0, \sigma^2 R),
\end{aligned}
\end{equation}
where $\mathcal N(0, \sigma^2 R)$ is a multivariate normal distribution with covariance $\sigma^2 R$ ($R$ is the correlation matrix). For convenience, we assume that random effects variance $\sigma^2$ is constant among response variables.
Equivalently, we can write this as
\begin{equation}
\begin{aligned}
\mathbf y_i &= 1(\mathbf z_i > 0)\\
\mathbf z_i &\sim \mathrm{logistic}(\pmb \mu + \mathbf r_i, 1)\\
\mathbf r_i &\sim \mathcal{N}(0, \sigma^2 R),
\end{aligned}
\end{equation}
which can be further expanded as 
\begin{equation}
\begin{aligned}
\mathbf y_i &= 1(\mathbf z_i > 0)\\
\mathbf z_i &= \pmb \mu + \mathbf r_i + \pmb \epsilon_i\\
\mathbf r_i &\sim \mathcal{N}(0, \sigma^2 R)\\
\epsilon_{ij} &\sim \mathrm{logistic}(0, 1)
\end{aligned}
\end{equation}
Essentially, we have a continuous latent variable $\mathbf z_i$ which has a mean $\pmb \mu$ and two ``error'' terms: $\mathbf r_i$, which follows a multivariate normal with covariance $\sigma^2 R$, and $\pmb \epsilon_i$, which follows an independent logistic (each marginal distribution is an i.i.d. logistic distribution).
Due to two levels of uncertainties, it becomes much harder to estimate the correlation structure $R$.
I don't have a good analytical argument for this but I hope this is somewhat intuitive... I'll compare this with other models later; this might make things slightly clearer.

\subsubsection*{Identifiability of the random effects variance}

It doesn't seem like Jonathan is completely convinced that $\sigma^2$ is not identifiable; he says that it is ``practically'' unidentifiable. Let's try to do some math. Consider a univariate logistic regression with an underlying normal random effects on the mean:
\begin{equation}
\begin{aligned}
y_i &= 1(z_i > 0)\\
z_i &\sim \mathrm{logistic}(\mu + r_i, 1)\\
r_i &\sim \mathcal{N}(0, \sigma^2)
\end{aligned}
\end{equation}
Then, the marginal likelihood of this model can be written as 
\begin{equation}
\begin{aligned}
\prod_{i=1}^{n} \int_{-\infty}^\infty \int_{-\infty}^\infty \left\{1(z_i>0)^{y_i} 1(z_i \leq 0)^{1 - y_i} \right\} f(z_i | \mu + r_i, 1) dz_i g(r_i|0, \sigma^2) dr_i,
\end{aligned}
\end{equation}
where $f$ is the pdf of the standard logistic distribution and $g$ is the pdf of the standard normal.
This is ugly. 
When $y_i = 0$, we have 
\begin{equation}
\begin{aligned}
&\int_{-\infty}^\infty \left\{1(z_i>0)^{y_i} 1(z_i \leq 0)^{1 - y_i} \right\} f(z_i | \mu + r_i, 1) dz_i\\
&= \int_{-\infty}^0 f(z_i | \mu + r_i, 1) dz_i\\
&= \frac{1}{1 + \exp(\mu + r_i)}
\end{aligned}
\end{equation}
Then,
\begin{equation}
\begin{aligned}
&\int_{-\infty}^\infty \int_{-\infty}^\infty \left\{1(z_i>0)^{y_i} 1(z_i \leq 0)^{1 - y_i} \right\} f(z_i | \mu + r_i, 1) dz_i g(r_i|0, \sigma^2) dr_i\\
&= \frac{1}{\sqrt{2\pi\sigma^2}}  \int_{-\infty}^\infty \frac{1}{1 + \exp(\mu + r_i)} \exp\left(- \frac{r_i^2}{2\sigma^2} \right) dr_i
\end{aligned}
\end{equation}
I can't evaluate this integral analytically but eventually the marginal likelihood can be written as 
\begin{equation}
\begin{aligned}
&\left(\frac{1}{\sqrt{2\pi\sigma^2}}\right)^n \left(\int_{-\infty}^\infty \frac{1}{1 + \exp(\mu + r_i)} \exp\left(- \frac{r_i^2}{2\sigma^2} \right) dr_i\right)^{n_0} \\
&\times \left(\int_{-\infty}^\infty \left(1- \frac{1}{1 + \exp(\mu + r_i)}\right) \exp\left(- \frac{r_i^2}{2\sigma^2} \right) dr_i\right)^{n_1},
\end{aligned}
\end{equation}
where $n_0$ is the number of 0's and $n_1$ is the number of 1's. 

We can work out a numerical example. Assume $n_1 = 60$ and $n_0=40$. Then, the MLE of $\mu$ of the logistic regression without random effects is approximately 0.4 (plogis(0.4) is approximately 0.6). As we increase $\sigma$, we see that our estimate of $\mu$ increases. Here, we show the log marginal likelihood surface:
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlkwd{library}\hlstd{(emdbook)}
\hlstd{ifun1} \hlkwb{<-} \hlkwa{function}\hlstd{(}\hlkwc{r}\hlstd{,} \hlkwc{mu}\hlstd{,} \hlkwc{sigma}\hlstd{)}
        \hlnum{1}\hlopt{/}\hlstd{(}\hlnum{1} \hlopt{+} \hlkwd{exp}\hlstd{(mu} \hlopt{+} \hlstd{r))} \hlopt{*} \hlkwd{exp}\hlstd{(}\hlopt{-}\hlstd{r}\hlopt{^}\hlnum{2}\hlopt{/}\hlstd{(}\hlnum{2} \hlopt{*} \hlstd{sigma}\hlopt{^}\hlnum{2}\hlstd{))}
\hlstd{ifun2} \hlkwb{<-} \hlkwa{function}\hlstd{(}\hlkwc{r}\hlstd{,} \hlkwc{mu}\hlstd{,} \hlkwc{sigma}\hlstd{)}
        \hlstd{(}\hlnum{1} \hlopt{-} \hlnum{1}\hlopt{/}\hlstd{(}\hlnum{1} \hlopt{+} \hlkwd{exp}\hlstd{(mu} \hlopt{+} \hlstd{r)))} \hlopt{*} \hlkwd{exp}\hlstd{(}\hlopt{-}\hlstd{r}\hlopt{^}\hlnum{2}\hlopt{/}\hlstd{(}\hlnum{2} \hlopt{*} \hlstd{sigma}\hlopt{^}\hlnum{2}\hlstd{))}

\hlstd{llfun} \hlkwb{<-} \hlkwa{function}\hlstd{(}\hlkwc{mu}\hlstd{,} \hlkwc{sigma}\hlstd{,} \hlkwc{n1}\hlstd{=}\hlnum{60}\hlstd{,} \hlkwc{n0}\hlstd{=}\hlnum{40}\hlstd{,} \hlkwc{log}\hlstd{=}\hlnum{TRUE}\hlstd{) \{}
        \hlstd{first} \hlkwb{<-} \hlkwd{integrate}\hlstd{(ifun1,} \hlopt{-}\hlnum{200}\hlstd{,} \hlnum{200}\hlstd{,} \hlkwc{mu}\hlstd{=mu,} \hlkwc{sigma}\hlstd{=sigma)}
        \hlstd{second} \hlkwb{<-} \hlkwd{integrate}\hlstd{(ifun2,} \hlopt{-}\hlnum{200}\hlstd{,} \hlnum{200}\hlstd{,} \hlkwc{mu}\hlstd{=mu,} \hlkwc{sigma}\hlstd{=sigma)}

        \hlstd{ll} \hlkwb{<-} \hlstd{(n0} \hlopt{+} \hlstd{n1)} \hlopt{*} \hlkwd{log}\hlstd{(}\hlnum{1}\hlopt{/}\hlkwd{sqrt}\hlstd{(}\hlnum{2} \hlopt{*} \hlstd{pi} \hlopt{*} \hlstd{sigma}\hlopt{^}\hlnum{2}\hlstd{))} \hlopt{+}
                \hlstd{n0} \hlopt{*} \hlkwd{log}\hlstd{(first[[}\hlnum{1}\hlstd{]])} \hlopt{+} \hlstd{n1} \hlopt{*} \hlkwd{log}\hlstd{(second[[}\hlnum{1}\hlstd{]])}

        \hlkwa{if} \hlstd{(}\hlopt{!}\hlstd{log) ll} \hlkwb{<-} \hlkwd{exp}\hlstd{(ll)}

        \hlstd{ll}
\hlstd{\}}

\hlstd{muvec} \hlkwb{<-} \hlkwd{seq}\hlstd{(}\hlnum{0.35}\hlstd{,} \hlnum{3}\hlstd{,} \hlkwc{by}\hlstd{=}\hlnum{0.05}\hlstd{)}
\hlstd{sigmavec} \hlkwb{<-} \hlkwd{exp}\hlstd{(}\hlkwd{seq}\hlstd{(}\hlkwd{log}\hlstd{(}\hlnum{1}\hlstd{),} \hlkwd{log}\hlstd{(}\hlnum{10}\hlstd{),} \hlkwc{length.out}\hlstd{=}\hlnum{100}\hlstd{))}

\hlkwd{contour}\hlstd{(muvec, sigmavec,} \hlkwd{apply2d}\hlstd{(llfun, muvec, sigmavec),}
                \hlkwc{xlab}\hlstd{=}\hlkwd{expression}\hlstd{(mu),}
                \hlkwc{ylab}\hlstd{=}\hlkwd{expression}\hlstd{(sigma),}
                \hlkwc{main}\hlstd{=}\hlstr{"log (marginal) likelihood surface"}\hlstd{,}
                \hlkwc{levels}\hlstd{=}\hlkwd{c}\hlstd{(}\hlopt{-}\hlnum{67.31}\hlstd{,} \hlopt{-}\hlnum{67.4}\hlstd{,} \hlopt{-}\hlnum{68}\hlstd{,} \hlopt{-}\hlnum{70}\hlstd{,} \hlopt{-}\hlnum{80}\hlstd{))}
\end{alltt}
\end{kframe}
\includegraphics[width=\maxwidth]{figure/unnamed-chunk-1-1} 

\end{knitrout}

We can see that there's a relatively flat region starting from $\sigma \approx 0$ and $\mu \approx 0.5$ (see line -67.31).

\subsection{Probit regression}

Probit regression provides a natural way of defining the underlying correlation for multivariate binomial response:
\begin{equation}
\begin{aligned}
\mathbf y_i &= 1(\mathbf z_i > 0)\\
\mathbf z_i &\sim \mathcal{N}(\pmb \mu, \mathbf R)\\
\end{aligned}
\end{equation}
In other words,
\begin{equation}
\begin{aligned}
\mathbf y_i &= 1(\mathbf z_i > 0)\\
\mathbf z_i &\sim \mathcal{N}(\pmb \mu, \mathbf R)\\
\end{aligned}
\end{equation}
which can be rewritten as
\begin{equation}
\begin{aligned}
\mathbf y_i &= 1(\mathbf z_i > 0)\\
\mathbf z_i &= \pmb \mu + \pmb \epsilon_i \\
\pmb \epsilon_i &\sim \mathcal N(0, \mathbf R).
\end{aligned}
\end{equation}
This model tries to capture the correlation among the latent ``residuals''.
Compare this expression with the mixed model approach:
\begin{equation}
\begin{aligned}
\mathbf y_i &= 1(\mathbf z_i > 0)\\
\mathbf z_i &= \pmb \mu + \mathbf r_i + \pmb \epsilon_i\\
\mathbf r_i &\sim \mathcal{N}(0, \sigma^2 R)\\
\epsilon_{ij} &\sim \mathrm{logistic}(0, 1)
\end{aligned}
\end{equation}
The mixed model approach seeks to decompose the latent residual into two terms. We're going to have less power to detect the correlation structure.


\subsection{Multivariate logistic distribution}

Multivariate logistic distribution suggested by O'Brien allows us to model residual correlations while preserving the marginal logistic distribution:
\begin{equation}
\begin{aligned}
\mathbf y_i &= 1(\mathbf z_i > 0)\\
\mathbf z_i &= \pmb \mu + \log \left(\frac{F(\mathbf e_i)}{1 - F(\mathbf e_i)} \right) \\
\end{aligned}
\end{equation}
where $\mathbf e_i$ comes from a multivariate distribution with mean 0 and some correlation structure and $F$ is the univariate cumulative distribution function of $\mathbf e_i$. Then, the resulting distribution of $\mathbf z_i$ also has a very similar correlation structure (see code below) as $\mathbf e_i$ and each $z_{ij}$ follows a logistic distribution.

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlcom{## very similar correlations}
\hlkwd{library}\hlstd{(mvtnorm)}
\end{alltt}


{\ttfamily\noindent\color{warningcolor}{\#\# Warning: package 'mvtnorm' was built under R version 3.5.2}}

{\ttfamily\noindent\itshape\color{messagecolor}{\#\# \\\#\# Attaching package: 'mvtnorm'}}

{\ttfamily\noindent\itshape\color{messagecolor}{\#\# The following object is masked from 'package:emdbook':\\\#\# \\\#\#\ \ \ \  dmvnorm}}\begin{alltt}
\hlstd{corr} \hlkwb{<-} \hlkwd{matrix}\hlstd{(}
        \hlkwd{c}\hlstd{(}\hlnum{1}\hlstd{,} \hlnum{.1}\hlstd{,} \hlnum{.3}\hlstd{,}
          \hlnum{.1}\hlstd{,} \hlnum{1}\hlstd{,} \hlopt{-}\hlnum{.3}\hlstd{,}
          \hlnum{.3}\hlstd{,} \hlopt{-}\hlnum{.3}\hlstd{,} \hlnum{1}\hlstd{),}
        \hlnum{3}\hlstd{,} \hlnum{3}
\hlstd{)}

\hlkwd{set.seed}\hlstd{(}\hlnum{101}\hlstd{)}
\hlstd{rr} \hlkwb{<-} \hlkwd{rmvnorm}\hlstd{(}\hlnum{10000}\hlstd{,} \hlkwc{sigma}\hlstd{=corr)}
\hlkwd{cor}\hlstd{(rr)}
\end{alltt}
\begin{verbatim}
##            [,1]        [,2]       [,3]
## [1,] 1.00000000  0.09329252  0.2929604
## [2,] 0.09329252  1.00000000 -0.3104384
## [3,] 0.29296044 -0.31043837  1.0000000
\end{verbatim}
\begin{alltt}
\hlkwd{cor}\hlstd{(}\hlkwd{log}\hlstd{(}\hlkwd{pnorm}\hlstd{(rr)}\hlopt{/}\hlstd{(}\hlnum{1} \hlopt{-} \hlkwd{pnorm}\hlstd{(rr))))}
\end{alltt}
\begin{verbatim}
##           [,1]       [,2]       [,3]
## [1,] 1.0000000  0.0926942  0.2898303
## [2,] 0.0926942  1.0000000 -0.3088695
## [3,] 0.2898303 -0.3088695  1.0000000
\end{verbatim}
\end{kframe}
\end{knitrout}

Regardless of what model one decides to use, there should be a way to convert the estimate into a single, consistent scale...? Predict probability from posterior and convert that into odds ratio...? Not sure yet.



\bibliography{logit}

\end{document}
